{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from re import search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## old strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a load data csv's manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ## Path of the images\n",
    "\n",
    "# path = '/home/ubuntu/bucket/projects/2017_10_19_Profiling_rare_ORFs/20200303_96W_CP157A/20X_CP_CP157A_2'\n",
    "\n",
    "# first = \"20X_CP_CP157A_3_MMStack_\"\n",
    "\n",
    "# plate = '20X_CP_CP157A_2'\n",
    "\n",
    "\n",
    "# Well =  string.ascii_uppercase[0:7]\n",
    "# Field = range(1,13)\n",
    "# Sites = range(0,25)\n",
    "\n",
    "# def make_load_datacsvs():\n",
    "#     lst=[]\n",
    "    \n",
    "#     for w in Well:\n",
    "#         for f in Field:\n",
    "#             for s in Sites:\n",
    "#                 ch0 = first + str(w)+ str(f)+ \"-Site_\"+ str(s) + \".ome_ch_0.tif\"\n",
    "#                 ch1 = first + str(w)+ str(f)+ \"-Site_\"+ str(s) + \".ome_ch_1.tif\"\n",
    "#                 ch2 = first + str(w)+ str(f)+ \"-Site_\"+ str(s) + \".ome_ch_2.tif\"\n",
    "#                 ch3 = first + str(w)+ str(f)+ \"-Site_\"+ str(s) + \".ome_ch_3.tif\"\n",
    "#                 wellcolumn = str(w) + str(f)\n",
    "#                 sitecolumn = str(s)\n",
    "#                 lst.append([ch0, ch1, ch2, ch3, wellcolumn, sitecolumn])\n",
    "                \n",
    "#     df = pd.DataFrame(ls, columns = ['Filename_OrigDNA',\n",
    "#                                 'Filename_OrigProtein',\n",
    "#                                 'Filename_OrigMito',\n",
    "#                                 'Filename_OrigER',\n",
    "#                                 'Metadata_Well', \n",
    "#                                 'Metadata_Site'])\n",
    "    \n",
    "    \n",
    "#     return df\n",
    "    \n",
    "# data = make_load_datacsvs()                \n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Plate name and Path should be manaually added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pl = [plate] * data.shape[0]\n",
    "# pl = pd.DataFrame(plat, columns=[\"Metadata_Plate\"])\n",
    "# p_path = [path] * data.shape[0]\n",
    "# ch0_path = pd.DataFrame(p_path, columns=[\"FilePath_OrigDNA\"])\n",
    "# ch1_path = pd.DataFrame(p_path, columns=[\"FilePath_OrigProtein\"])\n",
    "# ch2_path = pd.DataFrame(p_path, columns=[\"FilePath_OrigMito\"])\n",
    "# ch3_path = pd.DataFrame(p_path, columns=[\"FilePath_OrigER\"])\n",
    "\n",
    "\n",
    "# final = pd.concat([df, pl, ch0_path, ch1_path, ch2_path, ch3_path], axis=1)\n",
    "\n",
    "\n",
    "# df.to_csv('/Users/habbasi/Desktop/load_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Method of creating manually load_data.csv and load_data_with_illum.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from re import search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/ubuntu/bucket/projects'\n",
    "projdir = '2017_10_19_Profiling_rare_ORFs'\n",
    "batchname = '20200615_96W_CP173'\n",
    "platename = '20X_CP_CP173_2'\n",
    "\n",
    "\n",
    "batchpath = path + \"/\" + projdir + \"/\" + batchname\n",
    "fpath = batchpath + \"/\" + platename\n",
    "outpath = path + \"/\" + projdir + \"/\" + 'workspace' + '/' + 'load_data_csv' + '/' + batchname + '/' + platename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makedirectory():\n",
    "    try:\n",
    "        os.makedirs(outpath)\n",
    "        print(\"Directory\", outpath, \"is created\")\n",
    "    except IOError:\n",
    "        print(\"Directory\", outpath, \"already exists\")\n",
    "        \n",
    "directory = makedirectory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/bucket/projects/2017_10_19_Profiling_rare_ORFs/workspace/load_data_csv/20200615_96W_CP173/20X_CP_CP173_2'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "def load_data():\n",
    "    \n",
    "    lst=[]\n",
    "\n",
    "    for i, image in enumerate(os.listdir(fpath)):\n",
    "        imgpath = fpath + \"/\" + image\n",
    "        head, tail = os.path.split(imgpath)\n",
    "        lst.append(tail)\n",
    "   \n",
    "    \n",
    "\n",
    "    ch0 = [s for s in lst if \"_ch_0\" in s]\n",
    "    ch1 = [s for s in lst if \"_ch_1\" in s]\n",
    "    ch2 = [s for s in lst if \"_ch_2\" in s]\n",
    "    ch3 = [s for s in lst if \"_ch_3\" in s]\n",
    "\n",
    "    zippedlist = list(zip(ch0, ch1, ch2, ch3))\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(zippedlist, columns=['FileName_OrigDNA', \n",
    "                                        'FileName_OrigProtein', \n",
    "                                        'FileName_OrigMito',\n",
    "                                       'FileName_OrigER'])\n",
    "\n",
    "    df['PathName_OrigDNA'] = fpath\n",
    "    df['PathName_OrigProtein'] = fpath\n",
    "    df['PathName_OrigMito'] = fpath\n",
    "    df['PathName_OrigER'] = fpath\n",
    "    df['Metadata_Plate'] = platename\n",
    "    df['Metadata_Well'] = (df['FileName_OrigDNA']\n",
    "                       .str.split('_')\n",
    "                       .str.get(5)\n",
    "                       .str.split('-')\n",
    "                       .str.get(0)\n",
    "                       .apply(lambda x: x[0] + str(0) + x[1] if len(x) < 3 else x)\n",
    "                          )\n",
    "                           \n",
    "    df['Metadata_Site'] = (df.FileName_OrigDNA\n",
    "                       .str.split('_')\n",
    "                       .str.get(6)\n",
    "                       .str.split('.')\n",
    "                       .str.get(0))\n",
    "\n",
    "\n",
    "    colnames = ['FileName_OrigDNA', 'PathName_OrigDNA',\n",
    "             'FileName_OrigProtein', 'PathName_OrigProtein',\n",
    "             'FileName_OrigMito', 'PathName_OrigMito',\n",
    "             'FileName_OrigER', 'PathName_OrigER', 'Metadata_Plate', 'Metadata_Well', 'Metadata_Site']\n",
    "\n",
    "    return df.reindex(columns=colnames)\n",
    "\n",
    "loaddata = load_data()\n",
    "\n",
    "loaddata.to_csv(outpath + \"/\" + 'load_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_with_illum():\n",
    "    \n",
    "    lst=[]\n",
    "\n",
    "    for i, image in enumerate(sorted(os.listdir(fpath))):\n",
    "        imgpath = fpath + \"/\" + image\n",
    "        head, tail = os.path.split(imgpath)\n",
    "        img_name = os.path.splitext(tail)[0]\n",
    "        lst.append(tail)\n",
    "    \n",
    "\n",
    "    ch0 = [s for s in lst if \"_ch_0\" in s]\n",
    "    ch1 = [s for s in lst if \"_ch_1\" in s]\n",
    "    ch2 = [s for s in lst if \"_ch_2\" in s]\n",
    "    ch3 = [s for s in lst if \"_ch_3\" in s]\n",
    "\n",
    "    zippedlist = list(zip(ch0, ch1, ch2, ch3))\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(zippedlist, columns=['FileName_OrigDNA', \n",
    "                                        'FileName_OrigProtein', \n",
    "                                        'FileName_OrigMito',\n",
    "                                       'FileName_OrigER'])\n",
    "\n",
    "    df['PathName_OrigDNA'] = fpath\n",
    "    df['PathName_OrigProtein'] = fpath\n",
    "    df['PathName_OrigMito'] = fpath\n",
    "    df['PathName_OrigER'] = fpath\n",
    "    df['Metadata_Plate'] = platename\n",
    "    df['FileName_IllumDNA'] = platename+'_IllumDNA.mat'\n",
    "    df['PathName_IllumDNA'] = batchpath + '/' + 'illum' + platename\n",
    "    df['FileName_IllumProtein'] = platename+'_IllumProtein.mat'\n",
    "    df['PathName_IllumProtein'] = batchpath + '/' + 'illum' + platename\n",
    "    df['FileName_IllumMito'] = platename+'_IllumMito.mat'\n",
    "    df['PathName_IllumMito'] = batchpath + '/' + 'illum' + platename\n",
    "    df['FileName_IllumER'] = platename+'_IllumER.mat'\n",
    "    df['PathName_IllumER'] = batchpath + '/' + 'illum' + platename\n",
    "    \n",
    "    df['Metadata_Well'] = (df.FileName_OrigDNA\n",
    "                       .str.split('_')\n",
    "                       .str.get(5)\n",
    "                       .str.split('-')\n",
    "                       .str.get(0)\n",
    "                       .apply(lambda x: x[0] + str(0) + x[1] if len(x) < 3 else x)\n",
    "                          )\n",
    "    \n",
    "\n",
    "                           \n",
    "    df['Metadata_Site'] = (df.FileName_OrigDNA\n",
    "                       .str.split('_')\n",
    "                       .str.get(6)\n",
    "                       .str.split('.')\n",
    "                       .str.get(0)\n",
    "                          )\n",
    "\n",
    "\n",
    "    colnames = ['FileName_OrigDNA', 'PathName_OrigDNA',\n",
    "             'FileName_OrigProtein', 'PathName_OrigProtein',\n",
    "             'FileName_OrigMito', 'PathName_OrigMito',\n",
    "             'FileName_OrigER', 'PathName_OrigER', \n",
    "             'Metadata_Plate','Metadata_Well', \n",
    "            'Metadata_Site', 'FileName_IllumDNA', 'PathName_IllumDNA', \n",
    "            'FileName_IllumProtein', 'PathName_IllumProtein', \n",
    "            'FileName_IllumMito', 'PathName_IllumMito','FileName_IllumER','PathName_IllumER']\n",
    "\n",
    "    return df.reindex(columns=colnames)\n",
    "\n",
    "loaddataillum = load_data_with_illum()\n",
    "loaddataillum.to_csv(outpath + \"/\" + 'load_data_with_illum.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
